In collaboration with:

* The University of Tokyo

* IRCAM (Paris, France - Institut de Recherche et Coordination Acoustique/Musique).

&nbsp;

<p align="center"> <img src="https://raw.githubusercontent.com/adrienchaton/seminar_geidai_AI_Music/master/misc/Mandelbrot.jpeg" width="533" height="400"> </p>

&nbsp;


## Outline

Machine learning and deep neural networks have thrived through the last decades on increasing computational ressources, allowing for a steady development of new concepts along with a broad range of applications extending and complementing human knowledge. Computer vision and text processing are prominent topics in which many algorithms have been tested. They generalize to a certain extent to music language and sound, which nonetheless hold some key specificities that have themselves established as new reference topics to AI research.

The knowledge and experience of music are grounded into several layers of information. In the symbolic domain, one can account for compositional structures. However it is abstracted from the sound content and thus unmatched with a range of contemporary practices and performances that focus on sound production and perception. Forming and analysing consistent sctrutures accross scales ranging from streams of waveform samples (~ miliseconds) to musical events and pieces (seconds to minutes) has drawn many research efforts to close a gap, that is still open.

To address these questions, a constant dialogue has florished accross modalities. Pulling from general to music oriented developements and back. Successfully specifying some of these breakthroughs to sound domain and raising new challenges along with solutions that directly emerged from the intrinsic nature of sound. Be it on the level of music theory and signal processing, or to their resulting psychoacoustic qualities.

Given this extensive corpus of research and technologies, that is constantly refining, remains yet a challenge in implementing meaningful interactions for human creativity and production, such as applying to music composition and sound design. How will generative neural networks integrate in our practices ? How can AI enrich human art and foster new paradigms for music ?

This pannel of presentations followed with Q&A is an invitation to assess some of the latest developments of AI applied to music and sound, both from a computational and a composer view. Engaging computer science and music practitioners to discuss the specific potentialities that are arising at the interesection of their fields and beyond.

&nbsp;

---

人工知能と作曲2
研究会
東京芸術大学 千住キャンパス 
2020年1月19日

コラボレーション：東京大学、IRCAM

概要

機械学習とディープニューラルネットワークは、近年、益々性能を高めておりしており、人間の知識を拡張してくれるだけでなく、新しい概念による開発をも可能にしてくれている。コンピュータービジョンとテキスト処理は、既に広く知られている研究であろう。しかし、作曲と音響に関する人工知能の研究に関しては新たな課題である。
音楽に関する知識と経験は、様々な分野にまたがっている。作曲構造においては、音楽理論により説明することができる。 一方で、サウンドに関しては抽象的なものであるため、その生成と知覚に焦点を当てた範囲を考慮すると、前者とは随分異なるものと言えよう。 最近では、ミリ秒単位の波形サンプルから音楽イベントや曲（数秒から数分）に至るまで、スケール全体で一貫した構造を形成および分析するための多くの研究が行われている。
これにより、異なる分野間で盛んに対話が行われるようになった。 一般的な人工知能ではなく、音楽側の視点に立ち、 サウンドの本質的な性質により、新しい課題を提起するに至った。それは、音楽理論、信号処理、音響心理学まで、様々な分野に渡る。
この研究のためには大規模な技術が必要とされる課題に加えて、音楽の作曲やサウンドデザインへの応用など、人間の創造性と実践としての創作のために、意義のあるインタラクションを実装する課題は依然として残っている。果たして、ジェネラティブニューラルネットワークは私たちの実践をどのように統合してくれるのか？AIはどのようにして人間の芸術を豊かにし、音楽の新しいパラダイムを提示できるのか？ 
この研究会では、コンピューターサイエンスと作曲家側の両観点から、音楽とサウンドに適用される人工知能の最新の開発について再考する。 コンピュータサイエンスと音楽の専門家に、それぞれの分野にて共通する課題とその可能性についても議論する

&nbsp;


## Program

Titles, abstracts and schedule of the program are being prepared.

**16h30:** Welcome coffee

**17h-22h:** Presentations and Q&A with:

* Suguru Goto (Tokyo University of the Arts)

*The Technique of Contemporary Music Composition with using Deep Learning*

* Jie Man (Tokyo University of the Arts)

*Introduction of bach library in Max/MSP*

* Shinae Kang (Tokyo University of the Arts)

*A discussion of works created with the aid of AI and AI research trends for music*

* Jinwoong Kim (Tokyo University of the Arts)

*Orchidea, An intelligent assisted orchestration tool*

* Sachi Tanihara (Tokyo University of the Arts)

*AI composition and physicality --from Virtual Composer-Singer to Music Robots all over the world--*

* Philippe Esling (IRCAM/The University of Tokyo)

*ACIDS - Artificial Creative Intelligence*

* Daisuke Saito (The University of Tokyo)

*Modeling of chorus singing by a signal processing approach*

* Naotake Masuda (The University of Tokyo)

*Audio Synthesizer Control using Deep Generative Models and Normalizing Flows*

* Adrien Bitton (IRCAM/The University of Tokyo)

*Neural granular sound synthesis for raw waveform generation*

&nbsp;

[English program (under preparation)](https://drive.google.com/file/d/1Oth0-Ocoi_Lp-qfrAUL10kEkPh7H23Tt)

[Presentation materials](https://github.com/adrienchaton/seminar_geidai_AI_Music/tree/master/documents) will be provided prior to the start of the seminar.

---

[発表者](https://drive.google.com/file/d/1ebtbDYC9mbe5JxWUHgNsKsOoi04NNaO7/view?usp=sharing)

&nbsp;


## Details

**Date and time:** The 15th of January 2020, from 16h30 until 22h. <ins>Start of the first presentation at 17h</ins>.

**Place:** Tokyo University of the Arts, Department of Musical Creativity and the Environment, <ins>Senju Campus</ins>, The 1st conference room

〒 120-0034, 1-25-1 Senju, Adachi-ku, Tokyo.

[Access to Senju Campus](https://www.geidai.ac.jp/english/access#SenjuCampus)

Presentations will be given in English, however, Q&A may be possible in Japanese. We invite the discussions to be continued around a table in a neighboring Izakaya after 22h.

Following this seminar, a concert is being prepared for Februar by the Department of Musical Creativity and will be announced.

For information about the previous edition, please refer to the [1st edition webpage](https://tcmml.github.io).

**Contacts:** bitton@ircam.fr / goto.suguru@ms.geidai.ac.jp

&nbsp;

---

日時：2020年1月15日　16:30より22:00まで。 
プレゼンテーションは17:00より開始。 
場所: 東京芸術大学 音楽環境創造科　千住キャンパス　第一講義室
〒120-0034 東京都足立区千住1-25-1 東京藝術大学千住校地
「北千住駅」から徒歩5分（JR常磐線、東京メトロ千代田線、日比谷線、東武スカイツリーライン、つくばエクスプレス）
http://www.geidai.ac.jp/access/senju

お問い合わせ: 
bitton@ircam.fr 

＊プレゼンテーションは英語で行われます。
＊＊この研究会の後、2月19日に東京芸術大学にて人工知能で作曲された作品のコンサートが行われる予定。
＊＊＊前回の学会についてはこちらを参照して下さい。https://tcmml.github.io
＊＊＊＊一般より参加ご希望は、事前にご連絡を下さい。
Tel: 050-5525-2742
Email: mce-office@ml.geidai.ac.jp


<!---
The program of this seminar is being prepared and will be announced in the beginning of December.

It is a collaboration between:

* Tokyo University of the Arts (Geidai)

* The University of Tokyo (Todai)

* IRCAM (Paris, France - Institut de Recherche et Coordination Acoustique/Musique)

contact: bitton@ircam.fr

For information about the previous edition, please refer to the [1st edition webpage](https://tcmml.github.io)
-->

<!---

![Image](https://raw.githubusercontent.com/adrienchaton/seminar_geidai_AI_Music/master/misc/xmm_featured.jpg)

## Welcome to GitHub Pages

You can use the [editor on GitHub](https://github.com/adrienchaton/seminar_geidai_AI_Music/edit/master/README.md) to maintain and preview the content for your website in Markdown files.

Whenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.

### Markdown

Markdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for

```markdown
Syntax highlighted code block

# Header 1
## Header 2
### Header 3

- Bulleted
- List

1. Numbered
2. List

**Bold** and _Italic_ and `Code` text

[Link](url) and ![Image](src)
```

For more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).

### Jekyll Themes

Your Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/adrienchaton/seminar_geidai_AI_Music/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.

### Support or Contact

Having trouble with Pages? Check out our [documentation](https://help.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we’ll help you sort it out.

-->

